# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Documentation Structure

| Document | Purpose |
|----------|---------|
| `IDEA.md` | Brief concept statement - the "what" and "why" in 2-3 sentences |
| `DESIGN.md` | Complete technical specification - architecture, database schema, features, algorithms, configuration. The "how" in full detail. Reference this for implementation decisions. |
| `PLAN.md` | Phased implementation plan - task breakdown, dependencies, order of work. Track progress here. |
| `CLAUDE.md` | Quick reference for Claude Code - commands, architecture summary, key patterns |

When implementing features, consult DESIGN.md for specifications and update PLAN.md to track progress.

## Project Overview

AbaQuiz is a Telegram bot for BCBA exam preparation. It delivers daily Applied Behavior Analysis (ABA) quiz questions using Claude AI to generate questions from pre-processed BCBA study materials.

## Tech Stack

- Python 3.11+ with python-telegram-bot v20+
- Anthropic Claude API for question generation
- SQLite with aiosqlite for async database operations
- APScheduler for scheduled quiz delivery
- pdfplumber for PDF preprocessing
- Docker for deployment

## Development Environment

Always use the virtual environment (`.venv`) for all Python-related commands:

```bash
# Activate virtual environment
source .venv/bin/activate

# Or prefix commands with .venv/bin/
.venv/bin/python -m src.main
```

## Commands

```bash
# Run the bot
.venv/bin/python -m src.main

# Run the web admin interface
.venv/bin/python -m src.main --web-only

# Run preprocessing on BCBA PDFs (one-time)
.venv/bin/python -m src.preprocessing.run_preprocessing --input data/raw/ --output data/processed/

# Run tests
.venv/bin/pytest tests/

# Run single test file
.venv/bin/pytest tests/test_handlers.py

# Docker
docker-compose up --build
```

## Architecture

### Core Flow
1. **Scheduler** triggers at 8 AM/PM per user timezone
2. **Question Generator** loads content from `data/processed/*.md`, calls Claude API
3. **Bot Handler** sends question with inline keyboard buttons
4. **Callback Handler** processes answer, updates stats, awards points/achievements

### Middleware Stack
All handlers use decorator-based middleware in order:
```python
@dm_only_middleware        # Ignore group chats
@ban_check_middleware      # Block banned users
@rate_limit_middleware     # Throttle requests
```

### Key Modules
- `src/bot/handlers.py` - User command handlers (/start, /quiz, /stats)
- `src/bot/admin_handlers.py` - Admin commands (/ban, /broadcast, /usage)
- `src/bot/middleware.py` - Request filtering and access control
- `src/services/question_generator.py` - Claude API integration
- `src/services/scheduler.py` - APScheduler job setup
- `src/database/repository.py` - All database operations
- `src/gamification/` - Streaks, points, achievements logic

### Configuration
- Secrets in `.env` (TELEGRAM_BOT_TOKEN, ANTHROPIC_API_KEY)
- Settings in `config/config.json` (admin_users, rate limits, pricing)
- Config supports `${ENV_VAR}` substitution

### Database Tables
- `users` - Telegram users and subscription status
- `questions` - Generated quiz questions
- `user_answers` - Answer history
- `user_stats` - Points, streaks
- `achievements` - Unlocked badges
- `banned_users`, `admin_settings`, `api_usage` - Admin features

## PDF Preprocessing Pipeline

BCBA study PDFs go through a hybrid extraction process:
1. **Python extraction** (pdfplumber) - Extract text and tables
2. **Claude cleanup** (one-time) - Structure into markdown with proper headers
3. Output saved to `data/processed/` organized by BCBA Task List content area

## Question Generation

Questions are generated by loading relevant markdown content into Claude's context and requesting structured JSON output with question, options, correct_answer, explanation, and content_area fields.

### Question Categories
Questions are generated with variety using three categories:
- **Scenario-based (40%)**: Clinical vignettes requiring application of knowledge
- **Definition/Concept (30%)**: Key terms and principles testing
- **Application (30%)**: Novel situations requiring transfer of learning

### Question Pool Management

The pool manager (`src/services/pool_manager.py`) uses an active-user-based threshold system:

**Threshold Check**: Generate when avg unseen questions per active user < 20 (configurable)
- Active user = answered a question in the last 7 days
- Batch size = 50 questions when threshold is hit
- Distribution follows BCBA exam weights across 9 content areas

**Deduplication**: Uses Claude Haiku to check new questions against existing pool:
- Checks against same content area (50 most recent questions)
- Batches 5 existing questions per Haiku call for efficiency
- Skips dedup if Haiku call fails (logs warning, adds question anyway)

### Seeding Script

Seed initial questions with the CLI script:

```bash
# Generate 250 questions distributed by BCBA exam weights
python -m src.scripts.seed_questions --count 250

# Generate for specific content area only
python -m src.scripts.seed_questions --area "Ethics" --count 50

# Preview plan without generating (shows cost estimate)
python -m src.scripts.seed_questions --dry-run

# Fill gaps to reach target count
python -m src.scripts.seed_questions --resume --count 300
```

**Cost Estimate** (approximate):
- Initial seed (250 questions): ~$4.50
- Weekly batch (50 questions): ~$0.90

## Key Design Decisions

- **Question Pool**: Pre-generated and cached, batch refreshed daily (not generated on-demand)
- **Question Selection**: Hybrid - mostly random, 1-in-5 targets user weak areas (configurable)
- **Streaks**: Day-based - answer at least one question per day to maintain
- **Question Expiration**: None - users can answer old questions anytime
- **Onboarding**: Guided flow (timezone → focus areas → how-it-works → first question)
- **Privacy**: No leaderboards - individual progress only
- **Error Handling**: Retry 3x with backoff, notify admins on failure, never show errors to users
